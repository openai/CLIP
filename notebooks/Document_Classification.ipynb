{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Document Classification with CLIP\\n",
                "\\n",
                "This notebook demonstrates how to use CLIP for zero-shot document classification. We specifically address the common challenge of distinguishing between similar document types, such as **Receipts** and **Invoices**.\\n",
                "\\n",
                "We will show how providing more descriptive prompts (Prompt Engineering) helps the model differentiate between semantically similar categories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if they are not already installed\\n",
                "! pip install ftfy regex tqdm\\n",
                "! pip install git+https://github.com/openai/CLIP.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\\n",
                "import clip\\n",
                "from PIL import Image\\n",
                "import urllib.request\\n",
                "import io\\n",
                "\\n",
                "# Set up the device to use GPU if available, otherwise fallback to CPU\\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n",
                "print(f\"Using device: {device}\")\\n",
                "\\n",
                "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Loading a Sample Document\\n",
                "We will use a public image of a receipt to test the model. This image serves as a good example of a business document that can be ambiguous to classify."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/ReceiptSwiss.jpg/800px-ReceiptSwiss.jpg\"\\n",
                "\\n",
                "with urllib.request.urlopen(url) as url_response:\\n",
                "    image_data = url_response.read()\\n",
                "\\n",
                "# Preprocess the image to match CLIP's expected input format\\n",
                "image = preprocess(Image.open(io.BytesIO(image_data))).unsqueeze(0).to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Baseline: Simple Labels\\n",
                "First, we attempt to classify the image using simple, single-word labels. This often confuses the model because terms like \"receipt\" and \"invoice\" overlap significantly in meaning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "basic_categories = [\"receipt\", \"invoice\", \"form\", \"document\"]\\n",
                "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in basic_categories]).to(device)\\n",
                "\\n",
                "with torch.no_grad():\\n",
                "    # Calculate features for both image and text\\n",
                "    image_features = model.encode_image(image)\\n",
                "    text_features = model.encode_text(text_inputs)\\n",
                "\\n",
                "# Normalize the features\\n",
                "image_features /= image_features.norm(dim=-1, keepdim=True)\\n",
                "text_features /= text_features.norm(dim=-1, keepdim=True)\\n",
                "\\n",
                "# Calculate similarity scores\\n",
                "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\\n",
                "values, indices = similarity[0].topk(len(basic_categories))\\n",
                "\\n",
                "print(\"Baseline Results (Simple Prompts):\")\\n",
                "for value, index in zip(values, indices):\\n",
                "    print(f\"{basic_categories[index]:>16s}: {100 * value.item():.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Solution: Detailed Prompts\\n",
                "By adding descriptive details to our categories, we guide CLIP to focus on specific visual or structural characteristics. For example, specifying that an invoice is a \"full page\" document with \"billing details\" helps distinguish it from a small transaction receipt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "refined_categories = [\\n",
                "    \"a small sales receipt from a store transaction\",\\n",
                "    \"a full page commercial invoice with billing details\",\\n",
                "    \"a blank bureaucratic form\",\\n",
                "    \"a general text document\"\\n",
                "]\\n",
                "\\n",
                "text_inputs = torch.cat([clip.tokenize(c) for c in refined_categories]).to(device)\\n",
                "\\n",
                "with torch.no_grad():\\n",
                "    image_features = model.encode_image(image)\\n",
                "    text_features = model.encode_text(text_inputs)\\n",
                "\\n",
                "image_features /= image_features.norm(dim=-1, keepdim=True)\\n",
                "text_features /= text_features.norm(dim=-1, keepdim=True)\\n",
                "\\n",
                "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\\n",
                "values, indices = similarity[0].topk(len(refined_categories))\\n",
                "\\n",
                "print(\"\\\\nImproved Results (Detailed Prompts):\")\\n",
                "for value, index in zip(values, indices):\\n",
                "    print(f\"{refined_categories[index]:>45s}: {100 * value.item():.2f}%\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}